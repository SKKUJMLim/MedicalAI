from dataloader import get_dataloader
import torch
import torch.nn as nn
import torch.optim as optim
from models import resnet, vgg, combinedModel, clinicinfo
from sklearn.metrics import confusion_matrix
import seaborn as sns
from models import gradcam

from lime.lime_tabular import LimeTabularExplainer
from models import lime
import numpy as np
import matplotlib.pyplot as plt
import os


if __name__ == '__main__':

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("ì‚¬ìš© ì¥ì¹˜:", device)

    # ë‚œìˆ˜ ì‹œë“œ ê³ ì •
    torch.manual_seed(42)

    resize = 224
    # resize = 32
    mean = (0.485, 0.456, 0.406)
    std = (0.229, 0.224, 0.225)
    num_classes = 2
    batch_size = 8
    num_epochs = 1
    learning_rate = 0.001

    best_accuracy = 0.0
    best_model_path = './best_model.pth'

    train_loaders_dict, test_dataloader = get_dataloader(resize, mean, std, batch_size)

    print('==> Building model..')
    # resnet.test()
    preap_net = resnet.resnet34(pretrained=True)
    prelat_net= resnet.resnet34(pretrained=True)
    clinicinfo_net = clinicinfo.MLP(input_size=5, hidden_size=3, output_size=1)
    combined_model = combinedModel.CombinedResNet18(preap_net, prelat_net, clinicinfo_net, num_classes)

    # vgg.test()
    # preap_net = vgg.VGG('VGG19')
    # prelat_net = vgg.VGG('VGG19')
    # combined_model = combinedModel.CombinedVGG(preap_net, prelat_net, num_classes)

    combined_model.to(device=device)

    # ìµœì í™” ê¸°ë²• ì„¤ì •
    criterion = nn.CrossEntropyLoss()
    # optimizer = optim.SGD(combined_model.parameters(), lr=learning_rate)
    optimizer = optim.Adam(combined_model.parameters(), lr=learning_rate)

    # optimizer = optim.Adam([
    #     {'params': preap_net.parameters(), 'lr':learning_rate},
    #     {'params': prelat_net.parameters(), 'lr':learning_rate},
    # ])

    # í•™ìŠµ ë° ê²€ì¦ ì‹¤ì‹œ


    ## epoch ë£¨í”„
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch + 1, num_epochs))
        print("------------------------------")

        running_loss = 0.0
        combined_model.train()

        trainloader =  train_loaders_dict['train']

        for ids, preap_inputs, prelat_inputs, clinic_inputs, labels in trainloader:

            # GPUê°€ ì‚¬ìš©ê°€ëŠ¥í•˜ë©´ GPUì— ë°ì´í„° ì „ì†¡
            preap_inputs = preap_inputs.to(device)
            prelat_inputs = prelat_inputs.to(device)
            clinic_inputs = clinic_inputs.to(device)
            labels = labels.to(device)

            # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
            optimizer.zero_grad()
            outputs = combined_model(preap_inputs, prelat_inputs, clinic_inputs)

            '''1. ë‹¨ìˆœ cross-enropy loss'''
            loss = criterion(outputs, labels)

            '''2. Focal loss'''
            # Initialize Focal Loss
            # focal_loss = utils.FocalLoss(alpha=1, gamma=2, reduction='mean')
            # loss = focal_loss(outputs, labels)

            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(trainloader)}')

        # Validation
        combined_model.eval()
        correct = 0
        total = 0
        validationloader = train_loaders_dict['val']

        with torch.no_grad():
            for ids, preap_inputs, prelat_inputs, clinic_inputs, labels in validationloader:
                preap_inputs = preap_inputs.to(device)
                prelat_inputs = prelat_inputs.to(device)
                clinic_inputs = clinic_inputs.to(device)
                labels = labels.to(device)
                outputs = combined_model(preap_inputs, prelat_inputs, clinic_inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        print(f'Validation Accuracy: {accuracy}%')

        # Best ëª¨ë¸ ì €ì¥
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(combined_model.state_dict(), best_model_path)
            print(f'Saving model with accuracy: {best_accuracy}%')


    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€
    combined_model.load_state_dict(torch.load(best_model_path))
    combined_model.eval()
    correct = 0
    total = 0

    # ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (Confusion matrix)
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for ids, preap_inputs, prelat_inputs, clinic_inputs, labels in test_dataloader:
            # GPUê°€ ì‚¬ìš©ê°€ëŠ¥í•˜ë©´ GPUì— ë°ì´í„° ì „ì†¡
            preap_inputs = preap_inputs.to(device)
            prelat_inputs = prelat_inputs.to(device)
            clinic_inputs = clinic_inputs.to(device)
            labels = labels.to(device)
            combined_model.to(device)
            outputs = combined_model(preap_inputs, prelat_inputs, clinic_inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())  # ì‹¤ì œ ë¼ë²¨ ì €ì¥
            all_preds.extend(predicted.cpu().numpy())  # ì˜ˆì¸¡ ë¼ë²¨ ì €ì¥

    print(f'Accuracy of the best model on the test images: {100 * correct / total}%')

    '''Confusion Matrix ìƒì„±'''
    conf_matrix = confusion_matrix(all_labels, all_preds)

    # Confusion Matrix ì‹œê°í™” ë° ì´ë¯¸ì§€ ì €ì¥
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix for MedicalAI')

    # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥ (PNG í˜•ì‹)
    plt.savefig('confusion_matrix.png')

    # '''Grad-CAM'''
    # ## ì •ë©´ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ Grad-CAM
    # grad_cam = gradcam.GradCAM(model=combined_model.model1, target_layer=combined_model.model1.layer4)
    # gradcam.save_all_grad_cam_results(grad_cam=grad_cam, image_type='preap' , model=combined_model.model1, testloader=test_dataloader, combinedModel=combined_model)
    #
    # ## ì¸¡ë©´ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ Grad-CAM
    # grad_cam = gradcam.GradCAM(model=combined_model.model2, target_layer=combined_model.model2.layer4)
    # gradcam.save_all_grad_cam_results(grad_cam=grad_cam, image_type='prelat', model=combined_model.model2, testloader=test_dataloader, combinedModel=combined_model)

    '''LIME'''
    training_data = []
    for _, (_, _, _, clinic_inputs, _) in enumerate(test_dataloader):
        training_data.append(clinic_inputs.cpu().numpy())

    training_data = np.vstack(training_data)
    age_scaler, bmi_scaler, gender_encoder, side_encoder, presence_encoder = test_dataloader.dataset.get_scaler()

    # ğŸ¯ ë²”ì£¼í˜• Feature ì¸ë±ìŠ¤ ì„¤ì •
    categorical_features = [2, 3, 4]  # gender(2), side(3), presence(4)

    # ğŸ¯ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì›ë˜ ê°’ ì„¤ì • (LabelEncoder ì‚¬ìš©)
    categorical_names = {
        2: gender_encoder.classes_.tolist(),  # ğŸš€ LabelEncoderì—ì„œ ì§ì ‘ í´ë˜ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        3: side_encoder.classes_.tolist(),  # ğŸš€ LabelEncoderì—ì„œ ì§ì ‘ í´ë˜ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        4: presence_encoder.classes_.tolist()  # ğŸš€ LabelEncoderì—ì„œ ì§ì ‘ í´ë˜ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    }

    # ğŸ¯ LimeTabularExplainer ì´ˆê¸°í™”
    explainer = LimeTabularExplainer(
        training_data=training_data,
        feature_names=['age', 'bmi', 'gender', 'side', 'presence'],
        class_names=[0, 1],
        categorical_features=categorical_features,  # ğŸš€ ë²”ì£¼í˜• Feature ì¸ë±ìŠ¤ ì„¤ì •
        categorical_names=categorical_names,  # ğŸš€ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì›ë˜ ê°’ ì„¤ì •
        mode="classification"
    )

    # ì„¤ëª… ìƒì„±
    explanations = lime.explain_instance(test_dataloader, explainer, combined_model, device='cuda', max_samples=5000)
    lime.save_all_lime_results(explanations,
                               age_scaler=age_scaler,
                               bmi_scaler=bmi_scaler,
                               gender_encoder=gender_encoder,
                               side_encoder=side_encoder,
                               presence_encoder=presence_encoder)

